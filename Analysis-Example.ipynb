{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EPMT Analysis Example Notebook\n",
    "\n",
    "This notebook is intended to show how EPMT can be used to analyze performance characteristics (metadata) of FRE postprocessing (`frepp`) jobs, in addition to`refineDiag` and analysis scripts. The notebook walks through API queries to extract and display performance-related quantities for a given FRE experiment. This introduction only covers a small subset of EPMT's analysis capabilities, as it captures volumnous amounts of job, process, and thread-level data. \n",
    "\n",
    "EPMT-gathered metadata is stored in a database, with access provided by the EPMT Query API. The API provides functions that can return usable data in various formats: `pandas.DataFrame` objects (easy graphing and inspection), Python `dict` objects (dictionary, unique key/value pairs), job number lists (`terse`; fast), and `orm` database objects (fast/powerful/flexible/lazy). Data in these formats can then be rendered into plots/graphs/figures using standard Python libraries (e.g. `matplotlib`). EPMT also comes with some graphing functions for immediate use out-of-the-box, available in the `ui/` directory. Some of these will be used in this notebook.\n",
    "\n",
    "## Starting EPMT and opening this notebook\n",
    "To begin with this notebook, open a terminal and `ssh` in to any JHAN enabled analysis node (e.g. `an201`) and do:\n",
    "\n",
    "```\n",
    "module load epmt\n",
    "epmt notebook -- --no-browser --ip=`hostname` --notebook-dir=/home/First.Last/path/to/your/notebooks\n",
    "\n",
    "```\n",
    "Above, the `--no-browser` option is required to keep an internet browser from opening up immediately via `ssh` session, and the `--ip` option is helpful for printing a full URL for accessing these notebooks through a web browser on a local machine. Navigate to the output URL in any browser. What comes up should be the `notebook-dir` directory specified above. To begin working through this notebook then, click on `Analysis-Example.ipynb`.\n",
    "\n",
    "### Further Info\n",
    "Documentation of EPMT's Query and Graphics APIs can be seen in the `Query-API` and `Graphing-API` notebooks also in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Relevant Python Modules\n",
    "First, import the `epmt_query` and `ui.graphing` modules in order to access the relevant commands for retrieving data and plotting it. Additionally importing the `pandas` package allows the use of `DataFrame` objects, which are considered a convenient format for conducting data analysis. To import, click on the code cell below, then press `shift`+`enter` to execute the commands in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing epmt_query\n",
      "importing ui.graphing\n",
      "importing pandas\n"
     ]
    }
   ],
   "source": [
    "# import  epmt query and graphing modules\n",
    "print('importing epmt_query')\n",
    "import epmt_query as eq\n",
    "print('importing ui.graphing')\n",
    "import ui.graphing as gr\n",
    "\n",
    "# import pandas. optional but helpful 'display.max_columns' arg shows all DataFrame columns when printing\n",
    "print('importing pandas')\n",
    "import pandas\n",
    "pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving and Peeking at Job Metadata\n",
    "For this example notebook, let's grab the metadata of 1000 jobs currently in the DB. Note that in general EPMT job information is not stored permanenently, and old (>2 months) data are periodically removed. \n",
    "\n",
    "To get the desired metadata through EPMT, we use the `eq` object's `get_jobs` function with a limit argument `limit=1000`  One and a desired output format (`fmt='dict'`). One can also specify a desired tag for specific qualities of retrieved jobs, like experiment name (e.g. `tags='exp_name:c96_am4p0'`).\n",
    "\n",
    "<!-- \n",
    "For this example notebook, the metadata of a 2-year AM5 model run by user `Chris.Blanton`, with output corresponding to \n",
    "```\n",
    "/home/Chris.Blanton/am5/2022.01/c96L33_am4p0/gfdl.ncrc4-intel21-prod-openmp/stdout/postProcess\n",
    "```\n",
    "is provided. Note that in general EPMT job information is not stored permanenently, and old (>2 months) data are periodically removed. \n",
    "\n",
    "To get the metadata through EPMT, we use the `eq` object's `get_jobs` function with a experimental name tag (`tags='exp_name:c96_am4p0'`) and a desired output format (`fmt='dict'`). -->\n",
    "\n",
    "<!-- freppscripts, refineDiag, and analysis -->\n",
    "<!-- e.g. the pp jobs were tagged like (e.g. atmos 1980) -->\n",
    "<!-- exp_component:atmos; -->\n",
    "<!-- exp_name:c96L33_am4p0; -->\n",
    "<!-- exp_time:19800101; -->\n",
    "<!-- exp_platform:gfdl.ncrc4-intel21; -->\n",
    "<!-- exp_target:prod-openmp; -->\n",
    "<!-- exp_seg_months:12; -->\n",
    "<!-- script_name:c96L33 -->\n",
    "<!-- retrieve desired jobs to analyze -->\n",
    "<!-- g_am4p0_atmos_19800101' -->\n",
    "<!-- let's retrieve all jobs for one experiment -->\n",
    "<!-- modify this to retrieve jobs for another experiment -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements in jobs_all=0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve all jobs corresponding to below criteria/tag(s)\n",
    "# potential criteria are before/after job start/end/creation times, \n",
    "# potential tags are exp_component, exp_name, exp_time, exp_platform, and exp_target (i.e. command line args to frepp)\n",
    "# potential output formats are dict, pandas, terse, and orm\n",
    "jobs_all = eq.get_jobs(limit=200, before=-20,\n",
    "                       fmt='dict')\n",
    "print(f'number of elements in jobs_all={len(jobs_all)}')\n",
    "jobs_all[0]['start']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does one recorded job's worth of metadata look like? We can print out a single job entry in the usual way, but python yaml library can make the printout more digestible for little effort like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yaml import dump\n",
    "## uncomment me and run me- large output warning!\n",
    "print(dump(data=jobs_all[0], default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a further idea of what kind of data is in these jobs by `get_job_tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_all_tags=eq.get_job_tags(jobs=jobs_all)\n",
    "print(f'type(jobs_all_tags)={type(jobs_all_tags)}')\n",
    "for key in jobs_all_tags:print(f'key={key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we print a specific key in the `jobs_all_tags` dict, we see the values assigned to that key across jobs. The pritn statement below shows us that across these 1000 jobs, the `exp_platform` tag value took on only six different values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'jobs_all_tags[exp_platform]=\\n{jobs_all_tags[\"exp_platform\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about `exp_name`? In this case, there's a lot of possible values, and so here this approach is not as helpful. It might be wise to do some looping over the retried jobs to get the info we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'jobs_all_tags[exp_name]=\\n{jobs_all_tags[\"exp_name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Relevant Metadata \n",
    "\n",
    "Note that `get_jobs` is grabbing the oldest 1000 jobs *ingested by the DB*. To start picking at this, let's sort the jobs by experimental component (`exp_component`) while we exclude jobs from other users. One possible way of doing this is to loop over the dictionary data itself as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username of who's jobs we'd like to analyze, if desired\n",
    "name=None\n",
    "#name='Ian.Laflotte'\n",
    "### a convenient way to grab one's own username\n",
    "##import os\n",
    "##name = os.environ.get('USER')\n",
    "\n",
    "# sort jobs into refineDiag (rd), analysis (ana) and postprocessing (pp) categories\n",
    "jobs_rd = []\n",
    "jobs_ana = []\n",
    "jobs_pp = []\n",
    "\n",
    "found_exp_components={}\n",
    "found_exp_names={} # count times we find an exp_name\n",
    "\n",
    "for job in jobs_all:\n",
    "    \n",
    "    # skip jobs that are not the user's\n",
    "    if name is not None:\n",
    "        if job['user'] == name:\n",
    "            continue\n",
    "    \n",
    "    job_exp_name=job['tags']['exp_name']\n",
    "    exp_name_unique=True\n",
    "    for already_found_name in found_exp_names: #check if exp_name unique\n",
    "        if job_exp_name == already_found_name:\n",
    "            exp_name_unique=False\n",
    "            found_exp_names[already_found_name]+=1 #if not unique, add 1 to count\n",
    "            break\n",
    "            \n",
    "    if exp_name_unique: #if unique, new entry\n",
    "        found_exp_names[job_exp_name]=1\n",
    "\n",
    "    \n",
    "    job_exp_component=job['tags']['exp_component']\n",
    "    exp_component_unique=True\n",
    "    for already_found_component in found_exp_components: #check if exp_component unique\n",
    "        if job_exp_component == already_found_component:\n",
    "            exp_component_unique=False\n",
    "            found_exp_components[already_found_component]+=1 #if not unique, add 1 to count\n",
    "            break\n",
    "            \n",
    "    if exp_component_unique: #if unique, new entry\n",
    "        found_exp_components[job_exp_component]=1\n",
    "        \n",
    "        \n",
    "        \n",
    "    # then, separate the jobs into pp, refineDiag, and analysis jobs\n",
    "    if job_exp_component == 'refineDiag':\n",
    "        jobs_rd.append(job)\n",
    "    elif job_exp_component == 'analysis':\n",
    "        jobs_ana.append(job)\n",
    "    else:\n",
    "        ## if desired, print out exp_component of jobs other than the two above\n",
    "        #print('pp exp_component for this job is '+str(job['tags']['exp_component']))\n",
    "        jobs_pp.append(job)\n",
    "\n",
    "print(f'number of elements in jobs_rd ={len(jobs_rd )}')\n",
    "print(f'number of elements in jobs_ana={len(jobs_ana)}')\n",
    "print(f'number of elements in jobs_pp ={len(jobs_pp )}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'found {len(found_exp_components)} unique exp_components')\n",
    "sorted_found_exp_components= sorted(found_exp_components.items(),key=lambda x:x[1],reverse=True)\n",
    "print(f'top five:\\n{sorted_found_exp_components[:5]}')\n",
    "print('\\n')\n",
    "\n",
    "print(f'found {len(found_exp_names)} unique exp_names')\n",
    "sorted_found_exp_names= sorted(found_exp_names.items(),key=lambda x:x[1],reverse=True)\n",
    "print(f'top five:\\n{sorted_found_exp_names[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting/Graphing Job Metadata 1\n",
    "\n",
    "Now the fun part! A convenient function for making quick plots is the `gr` object's `graph_experiment` function, the code of which can be seen in file `ui/graphing.py`. `graph_experiment` will use `get_jobs` using the same experiment name as before, but only select the job numbers that also belong in `jobs_pp` (or `rd` or `ana` jobs). It uses the `pandas.DataFrame` format to average job times across experiment component, and then display the results as a bar graph. Run the below cells to show how wallclock (`duration`) and CPU times for each group of jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the type+name of a more-frequent exp_name\n",
    "choice=0\n",
    "print(type(sorted_found_exp_names[choice]))\n",
    "print(sorted_found_exp_names[choice])\n",
    "\n",
    "# it's a tupel, grab only the string\n",
    "target_exp_name=sorted_found_exp_names[choice][0]\n",
    "print(type(target_exp_name))\n",
    "print(target_exp_name)\n",
    "\n",
    "# title for plots\n",
    "base_title=f'Avg Wall / CPU Time, {target_exp_name}'\n",
    "print(f'base_title={base_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average duration and cpu_time by experimental component for post-processing jobs, \n",
    "# note: if this doesn't work, it might be that target_exp_name has no postprocessing.\n",
    "# in that case, set exp_name to a different one by changing the \"choice\" int, in the\n",
    "# previous field, or just typing your desired exp_name in the field below\n",
    "gr.graph_experiment(exp_name=target_exp_name, \n",
    "                    jobs=jobs_pp, \n",
    "                    metric=['duration','cpu_time'],\n",
    "                    title=f'{base_title} post-processing')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# average duration and cpu_time for refineDiag jobs\n",
    "# note: if this doesn't work, it might be that target_exp_name has no jobs of this component.\n",
    "# in that case, set exp_name to a different one by changing the \"choice\" int, or just typing\n",
    "# your desired exp_name in the field \n",
    "gr.graph_experiment(exp_name=target_exp_name, \n",
    "                    jobs=jobs_rd, \n",
    "                    metric=['duration','cpu_time'],\n",
    "                    title=f'{base_title} post-processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average duration and cpu_time for analysis jobs\n",
    "# note: if this doesn't work, it might be that target_exp_name has no jobs of this component.\n",
    "# in that case, set exp_name to a different one by changing the \"choice\" int, or just typing\n",
    "# your desired exp_name in the field \n",
    "gr.graph_experiment(exp_name=target_exp_name, \n",
    "                    jobs=jobs_ana, \n",
    "                    metric=['duration','cpu_time'],\n",
    "                    title=f'{base_title} analysis')\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting/Graphing Job Metadata 2\n",
    "\n",
    "Another function for making quick plots is the `gr` object's `graph_components` function, the code of which can also be seen in file `ui/graphing.py`. This function will do no averaging of the times, instead sorting them in descending order by wall clock time and displaying the results across individual job IDs.\n",
    "\n",
    "Below, the CPU time and duration are plotted for individual `refineDiag` jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration and cpu_time for refineDiag jobs by job ID\n",
    "# note: if this doesn't work, it might be that target_exp_name has no jobs of this component.\n",
    "# in that case, set exp_name to a different one by changing the \"choice\" int, or just typing\n",
    "# your desired exp_name in the field \n",
    "gr.graph_components(jobs=jobs_rd, \n",
    "                    exp_name=target_exp_name, \n",
    "                    exp_component='refineDiag', \n",
    "                    metric=['duration','cpu_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to do the same for jobs with `exp_component='analysis'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duration and cpu_time for analysis jobs by job ID\n",
    "#plot=gr.graph_components(jobs=jobs_ana, \n",
    "gr.graph_components(jobs=jobs_ana, \n",
    "                    exp_name=target_exp_name, \n",
    "                    exp_component='analysis', \n",
    "                    metric=['duration','cpu_time'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
