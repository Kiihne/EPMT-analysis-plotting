{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# canopy components by time\n",
    "breakdown one run of a experiment into its canopy componenets to figure out which are heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing epmt_query\n",
      "importing pandas\n"
     ]
    }
   ],
   "source": [
    "# import  epmt query \n",
    "print('importing epmt_query')\n",
    "import epmt_query as eq\n",
    "# import matplot for better plotting functions\n",
    "import sys\n",
    "sys.path.insert(0,'/home/Ian.Laflotte/ians_py374/pip_experiment')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import pandas. optional but helpful 'display.max_columns' arg shows all DataFrame columns when printing\n",
    "print('importing pandas')\n",
    "import pandas\n",
    "pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import epmt_query as eq\n",
    "\n",
    "def main(exp_name):\n",
    "    job_type_list=['analysis', 'clean', 'combine', 'data-catalog', 'make-timeavgs','mask-atmos-plevel','pp-starter','regrid-xy','remap-pp-components', 'rename-split-to-pp','split-netcdf','stage-history']\n",
    "    canopy_jobs=eq.get_jobs( fltr=(eq.Job.user_id == 'oar.gfdl.am5'), tags = 'exp_fre_mod:canopy;exp_name:'+exp_name, fmt = 'dict')\n",
    "    \n",
    "    \n",
    "    job_type_ids_dict={}\n",
    "    for job_type in job_type_list:\n",
    "        job_type_ids_dict[job_type]=[]\n",
    "        \n",
    "    print(job_type_ids_dict)\n",
    "        \n",
    "    for job in canopy_jobs:\n",
    "        jobname=job['jobname']\n",
    "        for job_type in job_type_list:\n",
    "            if job_type in jobname:\n",
    "                job_type_ids_dict[job_type].append(job['jobid'])\n",
    "                break\n",
    "    \n",
    "    return job_type_ids_dict\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [], 'clean': [], 'combine': [], 'data-catalog': [], 'make-timeavgs': [], 'mask-atmos-plevel': [], 'pp-starter': [], 'regrid-xy': [], 'remap-pp-components': [], 'rename-split-to-pp': [], 'split-netcdf': [], 'stage-history': []}\n"
     ]
    }
   ],
   "source": [
    "exp_name = 'c96L65_am5f5b7r0_pdclim2010F'\n",
    "canopy_ids = main(exp_name) \n",
    "keys = ['analysis', 'clean', 'combine', 'data-catalog', 'make-timeavgs','mask-atmos-plevel','pp-starter','regrid-xy','remap-pp-components', 'rename-split-to-pp','split-netcdf','stage-history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time for experiment  analysis c96L65_am5f5b7r0_pdclim2010F  is  9765.975646584  seconds with canopy. Duration is  660.807145424  seconds\n",
      "The total time for experiment  clean c96L65_am5f5b7r0_pdclim2010F  is  289.456589328  seconds with canopy. Duration is  5.659381968  seconds\n",
      "The total time for experiment  combine c96L65_am5f5b7r0_pdclim2010F  is  803.25413218  seconds with canopy. Duration is  46.341223128  seconds\n",
      "The total time for experiment  data-catalog c96L65_am5f5b7r0_pdclim2010F  is  90.896683476  seconds with canopy. Duration is  1.03300802  seconds\n",
      "The total time for experiment  make-timeavgs c96L65_am5f5b7r0_pdclim2010F  is  2507.507931068  seconds with canopy. Duration is  88.000034908  seconds\n",
      "The total time for experiment  mask-atmos-plevel c96L65_am5f5b7r0_pdclim2010F  is  684.381794204  seconds with canopy. Duration is  26.0961268  seconds\n",
      "The total time for experiment  pp-starter c96L65_am5f5b7r0_pdclim2010F  is  76.88919542  seconds with canopy. Duration is  1.133596528  seconds\n",
      "The total time for experiment  regrid-xy c96L65_am5f5b7r0_pdclim2010F  is  1137.96975688  seconds with canopy. Duration is  82.220500012  seconds\n",
      "The total time for experiment  remap-pp-components c96L65_am5f5b7r0_pdclim2010F  is  1852.647104516  seconds with canopy. Duration is  133.767473956  seconds\n",
      "The total time for experiment  rename-split-to-pp c96L65_am5f5b7r0_pdclim2010F  is  1807.281341992  seconds with canopy. Duration is  46.658790408  seconds\n",
      "The total time for experiment  split-netcdf c96L65_am5f5b7r0_pdclim2010F  is  649.411611504  seconds with canopy. Duration is  17.928202836  seconds\n",
      "The total time for experiment  stage-history c96L65_am5f5b7r0_pdclim2010F  is  38.692451996  seconds with canopy. Duration is  78.246526484  seconds\n"
     ]
    }
   ],
   "source": [
    "#sort all the data based on canopy or bronx\n",
    "for key in keys:\n",
    "    exp_jobs = eq.get_jobs(jobs = canopy_ids[key])\n",
    "    metrics_list = ['rssmax', 'num_procs', 'duration', 'write_bytes'] #designed for 4 metrics\n",
    "    bronx_jobs, canopy_jobs = [],[]\n",
    "    bronx_metrics,canopy_metrics = {}, {}\n",
    "    canopy_time, bronx_time = 0,0 \n",
    "    canopy_duration, bronx_duration = 0,0 \n",
    "\n",
    "    for metrics_key in metrics_list:  #set keys for dictionaries\n",
    "        bronx_metrics[metrics_key] = []\n",
    "        canopy_metrics[metrics_key] = []\n",
    "    #loop through once, and sort jobs into categories\n",
    "    for job in exp_jobs:\n",
    "        if job['tags']['exp_fre_mod'].find('canopy') >= 0:\n",
    "            canopy_jobs.append(job)\n",
    "            for metrics_key in metrics_list:\n",
    "                canopy_metrics[metrics_key].append(job[metrics_key])\n",
    "                canopy_time += job['cpu_time'] +job['time_waiting']\n",
    "                canopy_duration += job['duration']\n",
    "    #final calculations     \n",
    "    counts = [len(bronx_jobs), len(canopy_jobs)]        \n",
    "    averages = {'rssmax':[0,0],'num_procs':[0,0],'duration':[0,0],'write_bytes':[0,0]} #always ['bronx','canopy']\n",
    "    errors = {'rssmax':[0,0],'num_procs':[0,0],'duration':[0,0],'write_bytes':[0,0]}\n",
    "    for metrics_key in metrics_list:\n",
    "        averages[metrics_key][1] = sum(canopy_metrics[metrics_key])/len(canopy_jobs)\n",
    "        errors[metrics_key][1] = np.std(canopy_metrics[metrics_key])/(len(canopy_jobs))**.5\n",
    "    #print the info\n",
    "    print('The total time for experiment ', key, exp_name, ' is ', canopy_time/1e9, ' seconds with canopy. Duration is ',canopy_duration/1e9,' seconds')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
